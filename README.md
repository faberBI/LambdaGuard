<p align="center">ğŸ›¡ï¸ Î»-Guard

Overfitting detection for Gradient Boosting â€” no validation set required

<i>Understand when boosting stops learning signal and starts memorizing structure.</i>

</p>---

â“ Why Î»-Guard

In Gradient Boosting, overfitting usually appears after the real problem has already started.

Before validation error increases, the model is already:

- splitting the feature space into extremely small regions
- fitting leaves supported by very few observations
- becoming sensitive to tiny perturbations

The model is not improving prediction anymore.

It is learning the shape of the training dataset.

Î»-Guard detects that moment.

---

ğŸ§  The intuition

A boosting model learns two different things at the same time:

Component| What it does
Geometry| partitions the feature space
Predictor| assigns values to each region

Overfitting happens when:

Â«the geometry keeps growing but the predictor stops gaining real information.Â»

So Î»-Guard measures three signals:

- ğŸ“¦ capacity â†’ how complex the partition is
- ğŸ¯ alignment â†’ how much signal is extracted
- ğŸŒŠ stability â†’ how fragile predictions are

---

ğŸ§© Representation (the key object)

Every tree divides the feature space into leaves.

We record where each observation falls and build a binary matrix Z:

Z(i,j) = 1  if sample i falls inside leaf j
Z(i,j) = 0  otherwise

Rows â†’ observations
Columns â†’ all leaves across all trees

Think of Z as the representation learned by the ensemble.

Linear regression â†’ hat matrix H
Boosting â†’ representation matrix Z

---

ğŸ“¦ Capacity â€” structural complexity

C = Var(Z)

What it means:

- low C â†’ the model uses few effective regions
- high C â†’ the model fragments the space

When boosting keeps adding trees late in training, C grows fast.

---

ğŸ¯ Alignment â€” useful information

A = Corr(f(X), y)

(or equivalently the variance of predictions)

- high A â†’ trees add real predictive signal
- low A â†’ trees mostly refine boundaries

Important behavior:

Â«After some number of trees, alignment saturates.Â»

Boosting continues building structure even when prediction stops improving.

---

ğŸŒŠ Instability â€” sensitivity to perturbations

We slightly perturb inputs:

x' = x + Îµ
Îµ ~ Normal(0, ÏƒÂ²)

and measure prediction change:

S = average |f(x) âˆ’ f(x')|  /  prediction_std

- low S â†’ smooth model
- high S â†’ brittle model

This is the first thing that explodes during overfitting.

---

ğŸ”¥ The Overfitting Index

Î» = ( C / (A + C) ) Ã— S

Interpretation:

Situation| Î»
compact structure + stable predictions| low
many regions + weak signal| high
unstable predictions| very high

Î» measures:

Â«how much structural complexity is wasted.Â»

(You can normalize Î» to [0,1] for comparisons.)

---

ğŸ§ª Structural Overfitting Test

We can also check if specific training points dominate the model.

Approximate leverage:

H_ii â‰ˆ Î£_trees (learning_rate / leaf_size)

This behaves like regression leverage.

We compute:

T1 = mean(H_ii)        # global complexity
T2 = max(H_ii)/mean(H_ii)   # local memorization

Bootstrap procedure

repeat B times:
    resample training data
    recompute T1, T2

p-values:

p1 = P(T1_boot â‰¥ T1_obs)
p2 = P(T2_boot â‰¥ T2_obs)

Reject structural stability if:

p1 < Î±  OR  p2 < Î±

---

ğŸ“Š What Î»-Guard distinguishes

Regime| Meaning
âœ… Stable| smooth generalization
ğŸ“ˆ Global overfitting| too many effective parameters
âš ï¸ Local memorization| few points dominate
ğŸ’¥ Extreme| interpolation behavior

---

ğŸ§­ When to use

- monitoring boosting while trees are added
- hyperparameter tuning
- small datasets (no validation split)
- diagnosing late-stage performance collapse

---

ğŸ§¾ Conceptual summary

Z  â†’ learned representation
C  â†’ structural dimensionality
A  â†’ extracted signal
S  â†’ smoothness
Î»  â†’ structural overfitting

Overfitting = structure grows faster than information.

---

ğŸ“œ License

MIT (edit as needed)